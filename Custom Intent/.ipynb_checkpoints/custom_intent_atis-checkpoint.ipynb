{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67f55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169c77ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv (r'C:\\Users\\DhanrajChowdhury\\OneDrive - SPIKEWELL\\IntentClassifier\\Custom Intent\\atis_snips\\atis\\train.csv')\n",
    "df_test = pd.read_csv (r'C:\\Users\\DhanrajChowdhury\\OneDrive - SPIKEWELL\\IntentClassifier\\Custom Intent\\atis_snips\\atis\\test.csv')\n",
    "\n",
    "df_train = df_train[df_train[\"intent\"].str.contains(\"#\")==False]\n",
    "df_test = df_test[df_test[\"intent\"].str.contains(\"#\")==False]\n",
    "\n",
    "#df = df_train.append(df_test, ignore_index=True)\n",
    "df = pd.concat([df_train, df_test], ignore_index=True, sort=False)\n",
    "\n",
    "text = df['text']\n",
    "labels = df['intent']\n",
    "\n",
    "classes = np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "480950bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text        intent\n",
      "0     i want to fly from baltimore to dallas round trip   atis_flight\n",
      "1     round trip fares from baltimore to philadelphi...  atis_airfare\n",
      "2     show me the flights arriving on baltimore on j...   atis_flight\n",
      "3     what are the flights which depart from san fra...   atis_flight\n",
      "4     which airlines fly from boston to washington d...  atis_airline\n",
      "...                                                 ...           ...\n",
      "5825  please find all the flights from cincinnati to...   atis_flight\n",
      "5826  find me a flight from cincinnati to any airpor...   atis_flight\n",
      "5827  i'd like to fly from miami to chicago on ameri...   atis_flight\n",
      "5828  i would like to book a round trip flight from ...   atis_flight\n",
      "5829  find me a flight that flies from memphis to ta...   atis_flight\n",
      "\n",
      "[5830 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#convert_to_dataframe\n",
    "\n",
    "data = pd.DataFrame({'text': text, 'intent': labels})\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142499d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     text\n",
      "intent                   \n",
      "atis_abbreviation     180\n",
      "atis_aircraft          90\n",
      "atis_airfare          472\n",
      "atis_airline          195\n",
      "atis_airport           38\n",
      "atis_capacity          37\n",
      "atis_city              25\n",
      "atis_day_name           2\n",
      "atis_distance          30\n",
      "atis_flight          4298\n",
      "atis_flight_no         20\n",
      "atis_flight_time       55\n",
      "atis_ground_fare       25\n",
      "atis_ground_service   291\n",
      "atis_meal              12\n",
      "atis_quantity          54\n",
      "atis_restriction        6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(17, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None):\n",
    "    print(data.groupby(\"intent\").count())\n",
    "    \n",
    "data.groupby(\"intent\").count().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c860a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['atis_abbreviation', 'atis_aircraft', 'atis_airfare',\n",
       "       'atis_airline', 'atis_airport', 'atis_capacity', 'atis_city',\n",
       "       'atis_day_name', 'atis_distance', 'atis_flight', 'atis_flight_no',\n",
       "       'atis_flight_time', 'atis_ground_fare', 'atis_ground_service',\n",
       "       'atis_meal', 'atis_quantity', 'atis_restriction'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec8684de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_to_lower_case\n",
    "\n",
    "data[\"lower\"] = data.text.map(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "722be446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aa279d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_tokenize\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "data[\"tokenized\"] = data.lower.map(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76f6a0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords_remove\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "def remove_stop(strings, stop_list):\n",
    "    classed = [s for s in strings if s not in stop_list]\n",
    "    return classed\n",
    "\n",
    "stop = stopwords.words(\"english\")\n",
    "stop_punc = list(set(punctuation)) + stop\n",
    "\n",
    "data[\"selected\"] = data.tokenized.map(lambda df: remove_stop(df, stop_punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "869deefb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>intent</th>\n",
       "      <th>lower</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>selected</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i want to fly from baltimore to dallas round trip</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i want to fly from baltimore to dallas round trip</td>\n",
       "      <td>[i, want, to, fly, from, baltimore, to, dallas...</td>\n",
       "      <td>[want, fly, baltimore, dallas, round, trip]</td>\n",
       "      <td>[want, fly, baltimore, dallas, round, trip]</td>\n",
       "      <td>want fly baltimore dallas round trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>round trip fares from baltimore to philadelphi...</td>\n",
       "      <td>atis_airfare</td>\n",
       "      <td>round trip fares from baltimore to philadelphi...</td>\n",
       "      <td>[round, trip, fares, from, baltimore, to, phil...</td>\n",
       "      <td>[round, trip, fares, baltimore, philadelphia, ...</td>\n",
       "      <td>[round, trip, fare, baltimore, philadelphia, l...</td>\n",
       "      <td>round trip fare baltimore philadelphia le 1000...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>show me the flights arriving on baltimore on j...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>show me the flights arriving on baltimore on j...</td>\n",
       "      <td>[show, me, the, flights, arriving, on, baltimo...</td>\n",
       "      <td>[show, flights, arriving, baltimore, june, fou...</td>\n",
       "      <td>[show, flight, arriving, baltimore, june, four...</td>\n",
       "      <td>show flight arriving baltimore june fourteenth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what are the flights which depart from san fra...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>what are the flights which depart from san fra...</td>\n",
       "      <td>[what, are, the, flights, which, depart, from,...</td>\n",
       "      <td>[flights, depart, san, francisco, fly, washing...</td>\n",
       "      <td>[flight, depart, san, francisco, fly, washingt...</td>\n",
       "      <td>flight depart san francisco fly washington via...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which airlines fly from boston to washington d...</td>\n",
       "      <td>atis_airline</td>\n",
       "      <td>which airlines fly from boston to washington d...</td>\n",
       "      <td>[which, airlines, fly, from, boston, to, washi...</td>\n",
       "      <td>[airlines, fly, boston, washington, dc, via, c...</td>\n",
       "      <td>[airline, fly, boston, washington, dc, via, city]</td>\n",
       "      <td>airline fly boston washington dc via city</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5825</th>\n",
       "      <td>please find all the flights from cincinnati to...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>please find all the flights from cincinnati to...</td>\n",
       "      <td>[please, find, all, the, flights, from, cincin...</td>\n",
       "      <td>[please, find, flights, cincinnati, airport, n...</td>\n",
       "      <td>[please, find, flight, cincinnati, airport, ne...</td>\n",
       "      <td>please find flight cincinnati airport new york...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5826</th>\n",
       "      <td>find me a flight from cincinnati to any airpor...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>find me a flight from cincinnati to any airpor...</td>\n",
       "      <td>[find, me, a, flight, from, cincinnati, to, an...</td>\n",
       "      <td>[find, flight, cincinnati, airport, new, york,...</td>\n",
       "      <td>[find, flight, cincinnati, airport, new, york,...</td>\n",
       "      <td>find flight cincinnati airport new york city area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5827</th>\n",
       "      <td>i'd like to fly from miami to chicago on ameri...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i'd like to fly from miami to chicago on ameri...</td>\n",
       "      <td>[i, 'd, like, to, fly, from, miami, to, chicag...</td>\n",
       "      <td>['d, like, fly, miami, chicago, american, airl...</td>\n",
       "      <td>['d, like, fly, miami, chicago, american, airl...</td>\n",
       "      <td>'d like fly miami chicago american airline</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5828</th>\n",
       "      <td>i would like to book a round trip flight from ...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>i would like to book a round trip flight from ...</td>\n",
       "      <td>[i, would, like, to, book, a, round, trip, fli...</td>\n",
       "      <td>[would, like, book, round, trip, flight, kansa...</td>\n",
       "      <td>[would, like, book, round, trip, flight, kansa...</td>\n",
       "      <td>would like book round trip flight kansa city c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5829</th>\n",
       "      <td>find me a flight that flies from memphis to ta...</td>\n",
       "      <td>atis_flight</td>\n",
       "      <td>find me a flight that flies from memphis to ta...</td>\n",
       "      <td>[find, me, a, flight, that, flies, from, memph...</td>\n",
       "      <td>[find, flight, flies, memphis, tacoma]</td>\n",
       "      <td>[find, flight, fly, memphis, tacoma]</td>\n",
       "      <td>find flight fly memphis tacoma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5830 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text        intent  \\\n",
       "0     i want to fly from baltimore to dallas round trip   atis_flight   \n",
       "1     round trip fares from baltimore to philadelphi...  atis_airfare   \n",
       "2     show me the flights arriving on baltimore on j...   atis_flight   \n",
       "3     what are the flights which depart from san fra...   atis_flight   \n",
       "4     which airlines fly from boston to washington d...  atis_airline   \n",
       "...                                                 ...           ...   \n",
       "5825  please find all the flights from cincinnati to...   atis_flight   \n",
       "5826  find me a flight from cincinnati to any airpor...   atis_flight   \n",
       "5827  i'd like to fly from miami to chicago on ameri...   atis_flight   \n",
       "5828  i would like to book a round trip flight from ...   atis_flight   \n",
       "5829  find me a flight that flies from memphis to ta...   atis_flight   \n",
       "\n",
       "                                                  lower  \\\n",
       "0     i want to fly from baltimore to dallas round trip   \n",
       "1     round trip fares from baltimore to philadelphi...   \n",
       "2     show me the flights arriving on baltimore on j...   \n",
       "3     what are the flights which depart from san fra...   \n",
       "4     which airlines fly from boston to washington d...   \n",
       "...                                                 ...   \n",
       "5825  please find all the flights from cincinnati to...   \n",
       "5826  find me a flight from cincinnati to any airpor...   \n",
       "5827  i'd like to fly from miami to chicago on ameri...   \n",
       "5828  i would like to book a round trip flight from ...   \n",
       "5829  find me a flight that flies from memphis to ta...   \n",
       "\n",
       "                                              tokenized  \\\n",
       "0     [i, want, to, fly, from, baltimore, to, dallas...   \n",
       "1     [round, trip, fares, from, baltimore, to, phil...   \n",
       "2     [show, me, the, flights, arriving, on, baltimo...   \n",
       "3     [what, are, the, flights, which, depart, from,...   \n",
       "4     [which, airlines, fly, from, boston, to, washi...   \n",
       "...                                                 ...   \n",
       "5825  [please, find, all, the, flights, from, cincin...   \n",
       "5826  [find, me, a, flight, from, cincinnati, to, an...   \n",
       "5827  [i, 'd, like, to, fly, from, miami, to, chicag...   \n",
       "5828  [i, would, like, to, book, a, round, trip, fli...   \n",
       "5829  [find, me, a, flight, that, flies, from, memph...   \n",
       "\n",
       "                                               selected  \\\n",
       "0           [want, fly, baltimore, dallas, round, trip]   \n",
       "1     [round, trip, fares, baltimore, philadelphia, ...   \n",
       "2     [show, flights, arriving, baltimore, june, fou...   \n",
       "3     [flights, depart, san, francisco, fly, washing...   \n",
       "4     [airlines, fly, boston, washington, dc, via, c...   \n",
       "...                                                 ...   \n",
       "5825  [please, find, flights, cincinnati, airport, n...   \n",
       "5826  [find, flight, cincinnati, airport, new, york,...   \n",
       "5827  ['d, like, fly, miami, chicago, american, airl...   \n",
       "5828  [would, like, book, round, trip, flight, kansa...   \n",
       "5829             [find, flight, flies, memphis, tacoma]   \n",
       "\n",
       "                                             lemmatized  \\\n",
       "0           [want, fly, baltimore, dallas, round, trip]   \n",
       "1     [round, trip, fare, baltimore, philadelphia, l...   \n",
       "2     [show, flight, arriving, baltimore, june, four...   \n",
       "3     [flight, depart, san, francisco, fly, washingt...   \n",
       "4     [airline, fly, boston, washington, dc, via, city]   \n",
       "...                                                 ...   \n",
       "5825  [please, find, flight, cincinnati, airport, ne...   \n",
       "5826  [find, flight, cincinnati, airport, new, york,...   \n",
       "5827  ['d, like, fly, miami, chicago, american, airl...   \n",
       "5828  [would, like, book, round, trip, flight, kansa...   \n",
       "5829               [find, flight, fly, memphis, tacoma]   \n",
       "\n",
       "                                             normalized  \n",
       "0                  want fly baltimore dallas round trip  \n",
       "1     round trip fare baltimore philadelphia le 1000...  \n",
       "2        show flight arriving baltimore june fourteenth  \n",
       "3     flight depart san francisco fly washington via...  \n",
       "4             airline fly boston washington dc via city  \n",
       "...                                                 ...  \n",
       "5825  please find flight cincinnati airport new york...  \n",
       "5826  find flight cincinnati airport new york city area  \n",
       "5827         'd like fly miami chicago american airline  \n",
       "5828  would like book round trip flight kansa city c...  \n",
       "5829                     find flight fly memphis tacoma  \n",
       "\n",
       "[5830 rows x 7 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalize\n",
    "\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer, LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def normalize(text):\n",
    "    return \" \".join(text)\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "data[\"lemmatized\"] = data.selected.map(lambda xs: [lemmatizer.lemmatize(x) for x in xs])\n",
    "data[\"normalized\"] = data.lemmatized.apply(normalize)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "639f27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_test_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size = 0.15, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c4b77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf_tokenize\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "max_num_words = 10000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_num_words)\n",
    "tokenizer.fit_on_texts(train_data.normalized)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_data.normalized)\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_data.normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aad6a258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f24b63a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#max_len_of_sentences\n",
    "\n",
    "maxLen = 0\n",
    "for t in tokenized_train+tokenized_test:\n",
    "    if len(t) > maxLen:\n",
    "        maxLen = len(t)\n",
    "\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb916ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "train_padded = pad_sequences(tokenized_train, maxlen = maxLen, padding = \"post\")\n",
    "test_padded = pad_sequences(tokenized_test, maxlen = maxLen, padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b690f663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4955, 30) (875, 30)\n"
     ]
    }
   ],
   "source": [
    "print(train_padded.shape, test_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "300248fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_encoded_x\n",
    "\n",
    "def transform_x(data, tokenizer):\n",
    "    output_shape = [data.shape[0],\n",
    "                  data.shape[1],\n",
    "                  tokenizer.word_index.keys().__len__()]\n",
    "    results = np.zeros(output_shape)\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        for ii in range(data.shape[1]):\n",
    "            results[i, ii, data[i,ii]-1]= 1\n",
    "    return results\n",
    "\n",
    "xtr_transformed = transform_x(train_padded, tokenizer)\n",
    "xts_transformed = transform_x(test_padded, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "610c6ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4955, 30, 729)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtr_transformed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b92edc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_encoded_y\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder as OHE\n",
    "\n",
    "y_encoder = OHE().fit(np.array(train_data.intent).reshape(-1,1))\n",
    "\n",
    "ytr_encoded = y_encoder.transform(np.array(train_data.intent).reshape(-1,1)).toarray()\n",
    "yts_encoded = y_encoder.transform(np.array(test_data.intent).reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f734911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4955, 17)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytr_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea4a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index={}\n",
    "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263bf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "num_words = min(max_num_words, len(word_index))+1\n",
    "embedding_dim=len(embeddings_index['the'])\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_num_words:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f619b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #CNN\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Dropout\n",
    "# from tensorflow.keras.models import Model, Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "# embedding_layer = Embedding(num_words, embedding_matrix.shape[1], input_length=train_padded.shape[1], trainable=True)\n",
    "\n",
    "# sequence_input = Input(shape=(train_padded.shape[1],), dtype='int32')\n",
    "# embedded_sequences = embedding_layer(sequence_input)\n",
    "# x = Conv1D(64, 3, activation='relu')(embedded_sequences)\n",
    "# x = Conv1D(64, 3, activation='relu')(x)\n",
    "# x = MaxPooling1D(2)(x)\n",
    "# x = Flatten()(x)\n",
    "# x = Dense(100, activation='relu')(x)\n",
    "# preds = Dense(ytr_encoded.shape[1], activation='softmax')(x)\n",
    "# model = Model(sequence_input, preds)\n",
    "\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f64e89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Regular LSTM\n",
    "\n",
    "# from tensorflow.keras.layers import Input, Embedding, LSTM, Dense\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Embedding(num_words, embedding_matrix.shape[1], input_length=train_padded.shape[1],\n",
    "#           trainable=False, weights=[embedding_matrix]))\n",
    "\n",
    "# model.add(LSTM(256, return_sequences=True))\n",
    "# model.add(LSTM(128, return_sequences=False))\n",
    "# model.add(Dense(256, activation = \"relu\"))\n",
    "# model.add(Dense(128, activation = \"relu\"))\n",
    "# model.add(Dense(ytr_encoded.shape[1], activation = \"softmax\"))\n",
    "\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e928793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Bidirectional LSTM\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, Embedding\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Embedding(num_words, embedding_matrix.shape[1], input_length=train_padded.shape[1],\n",
    "#           trainable=False, weights=[embedding_matrix]))\n",
    "\n",
    "# model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "# model.add(LSTM(128, return_sequences=False))\n",
    "# model.add(Dense(256, activation = \"relu\"))\n",
    "# model.add(Dense(128, activation = \"relu\"))\n",
    "# model.add(Dense(ytr_encoded.shape[1], activation = \"softmax\"))\n",
    "\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dbfc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Bidirectional LSTM with Regularization\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, Embedding\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Embedding(num_words, embedding_matrix.shape[1], input_length=train_padded.shape[1], \n",
    "#           trainable=False, weights=[embedding_matrix]))\n",
    "    \n",
    "# model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "# model.add(LSTM(128, return_sequences=False))\n",
    "# model.add(Dense(256, activation = \"relu\", kernel_regularizer ='l1'))\n",
    "# model.add(Dense(128, activation = \"relu\", kernel_regularizer ='l1'))\n",
    "# model.add(Dense(ytr_encoded.shape[1], activation = \"softmax\"))\n",
    "\n",
    "\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339dfaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Bidirectional LSTM with Regularization and Dropout\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.python.keras.models import Sequential\n",
    "# from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, Embedding\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(Embedding(num_words, embedding_matrix.shape[1], input_length=train_padded.shape[1], \n",
    "#                     trainable=False, weights=[embedding_matrix]))\n",
    "    \n",
    "# model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(LSTM(128, return_sequences=False))\n",
    "# model.add(Dense(256, activation = \"relu\", kernel_regularizer ='l2'))\n",
    "# model.add(Dense(128, activation = \"relu\", kernel_regularizer ='l2'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(ytr_encoded.shape[1], activation = \"softmax\"))\n",
    "\n",
    "\n",
    "# model.compile(loss=\"categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ea97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bidirectional LSTM and Dropout(with Decaying LR)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, CuDNNLSTM, Activation, Bidirectional, Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, embedding_matrix.shape[1], input_length=train_padded.shape[1], \n",
    "                    trainable=False, weights=[embedding_matrix]))\n",
    "    \n",
    "model.add(Bidirectional(CuDNNLSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(CuDNNLSTM(128, return_sequences=False))\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(ytr_encoded.shape[1], activation = \"softmax\"))\n",
    "\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"Nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, LearningRateScheduler\n",
    "\n",
    "# tensorboard\n",
    "log_dir = os.path.join('logs')\n",
    "tb_callback = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"models/weights-improvement-{epoch:02d}-{accuracy:.2f}.ckpt\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='accuracy', verbose=1,\n",
    "                             save_weights_only=True, save_best_only=True, mode='max')\n",
    "\n",
    "# lr_scheduler\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.0005\n",
    "    drop = 0.5\n",
    "    epochs_drop = 5.0\n",
    "    \n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((epoch)/epochs_drop))\n",
    "    print(\"lr:\", lrate)\n",
    "    \n",
    "    return lrate\n",
    "    \n",
    "#     if lrate >= 0.0000125:\n",
    "#         return lrate\n",
    "#     else:\n",
    "#         return 0.0000125\n",
    "\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "\n",
    "callbacks_list = [lrate, tb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11ee51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    history = model.fit(train_padded, ytr_encoded, epochs = 50,\n",
    "                        batch_size = 64, shuffle=True,\n",
    "                        validation_split = 0.1, callbacks = callbacks_list)\n",
    "#with tf.device('/device:GPU:0'):\n",
    "#     history = model.fit(train_padded, ytr_encoded, epochs = 50,\n",
    "#                         batch_size = 64, shuffle=True,\n",
    "#                         validation_split = 0.1, callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821f8846",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58113725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# with tf.device('/cpu:0'):\n",
    "#     history = model.fit(train_padded, ytr_encoded, epochs = 5,\n",
    "#           batch_size = 64, shuffle=True,\n",
    "#           validation_split = 0.15)\n",
    "# #with tf.device('/device:GPU:0'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac8ad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b5096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_evaluation_train\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    prediction = y_encoder.inverse_transform(model.predict(train_padded))\n",
    "print(classification_report(train_data.intent, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ceff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance_evaluation_test\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    prediction_test = y_encoder.inverse_transform(model.predict(test_padded))\n",
    "print(classification_report(test_data.intent, prediction_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dfc582",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# cm = confusion_matrix(train_data.intent, prediction)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(25,25))\n",
    "# sns.set(font_scale=0.5)\n",
    "# sns.heatmap(cm, annot=True, linewidths=.5, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1110de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# incorr_fraction = 1 - np.diag(cm) / np.sum(cm, axis=1)\n",
    "\n",
    "# incorr_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2869647d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# incorr_fraction = 1 - np.diag(cm) / np.sum(cm, axis=1)\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(25,25))\n",
    "\n",
    "# plt.bar(np.arange(10), incorr_fraction)\n",
    "# plt.xlabel('True Label')\n",
    "# plt.ylabel('Fraction of incorrect predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2370dc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from keras.models import save_model\n",
    "\n",
    "model.save('models/my_model_atis')\n",
    "\n",
    "with open('utils/classes_atis.pkl','wb') as file:\n",
    "   pickle.dump(classes,file)\n",
    "\n",
    "with open('utils/tokenizer_atis.pkl','wb') as file:\n",
    "   pickle.dump(tokenizer,file)\n",
    "\n",
    "with open('utils/label_encoder_atis.pkl','wb') as file:\n",
    "   pickle.dump(y_encoder,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc515a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "# import pickle\n",
    "\n",
    "# model = keras.models.load_model('models/my_model_atis')\n",
    "\n",
    "# with open('utils/classes_atis.pkl','rb') as file:\n",
    "#   classes = pickle.load(file)\n",
    "\n",
    "# with open('utils/tokenizer_atis.pkl','rb') as file:\n",
    "#   tokenizer = pickle.load(file)\n",
    "\n",
    "# with open('utils/label_encoder_atis.pkl','rb') as file:\n",
    "#   y_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e663698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
