{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf42805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import pickle\n",
    "\n",
    "model = keras.models.load_model('models/my_model_custom')\n",
    "\n",
    "with open('utils/classes_custom.pkl','rb') as file:\n",
    "  classes = pickle.load(file)\n",
    "\n",
    "with open('utils/tokenizer_custom.pkl','rb') as file:\n",
    "  tokenizer = pickle.load(file)\n",
    "\n",
    "with open('utils/label_encoder_custom.pkl','rb') as file:\n",
    "  label_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb20d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras_preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5362186f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntentClassifier:\n",
    "    def __init__(self,classes,model,tokenizer,label_encoder):\n",
    "        self.classes = classes\n",
    "        self.classifier = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        \n",
    "    def remove_stop(self, strings, stop_list):\n",
    "        classed = [s for s in strings if s not in stop_list]\n",
    "        return classed\n",
    "    \n",
    "    def normalize(self, text):\n",
    "        return \" \".join(text)\n",
    "        \n",
    "    def clean(self, text):\n",
    "        #convert_to_dataframe\n",
    "        self.data = pd.DataFrame({'text': text})\n",
    "        #convert_to_lower_case\n",
    "        self.data[\"lower\"] = self.data.text.map(lambda x : x.lower())\n",
    "        #word_tokenize\n",
    "        self.data[\"tokenized\"] = self.data.lower.map(word_tokenize)\n",
    "        #stopwords_remove\n",
    "        stop = stopwords.words(\"english\")\n",
    "        stop_punc = list(set(punctuation)) + stop\n",
    "        self.data[\"selected\"] = self.data.tokenized.map(lambda df: self.remove_stop(df, stop_punc))\n",
    "        #normalize\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        self.data[\"lemmatized\"] = self.data.selected.map(lambda xs: [lemmatizer.lemmatize(x) for x in xs])\n",
    "        self.data[\"normalized\"] = self.data.lemmatized.apply(self.normalize)\n",
    "        return self.data.normalized\n",
    "\n",
    "    def sigmoid(self, scores):\n",
    "        self.scores = np.negative(scores)\n",
    "        self.exp = np.exp(self.scores)\n",
    "        self.scores = 1 / (1 + self.exp)\n",
    "        return self.scores\n",
    "    \n",
    "    def get_intent(self,text):\n",
    "        self.text = self.clean([text])\n",
    "        self.test_keras = self.tokenizer.texts_to_sequences(self.text)\n",
    "        self.test_keras_sequence = pad_sequences(self.test_keras, maxlen=12, padding='post')\n",
    "        self.pred = self.classifier.predict(self.test_keras_sequence)\n",
    "        #print(self.sigmoid(self.pred))\n",
    "        return self.label_encoder.inverse_transform(self.pred)[0][0]\n",
    "    #prediction_test = y_encoder.inverse_transform(model.predict(test_padded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f16c21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step\n",
      "Hardware Issue\n"
     ]
    }
   ],
   "source": [
    "nlu = IntentClassifier(classes,model,tokenizer,label_encoder)\n",
    "print(nlu.get_intent(\"the wifi is not showing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8dbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
